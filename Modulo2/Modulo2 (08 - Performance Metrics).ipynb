{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 01. Accuracy\n",
    "\n",
    "Compute the percentage of correct predictions **accuracy** (see [here](https://en.wikipedia.org/wiki/Sensitivity_and_specificity#Definitions)) for the following model output (`predicted`) and ground truth (`actual`).\n",
    "\n",
    "Execute the following cell to generate the data from which you must compute the metric. You may compute the metric implementing python code, or manually, or copy/pasting the actual and predicted data in Excel, etc.\n",
    "\n",
    "Observe that every time you execute the following cell, **a different set of values** is generated. You will have to compute the metric **for the values that you see**. If you run the cell again you will have to compute your metric value again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "t1_actual    = np.random.randint(2, size=20)\n",
    "t1_predicted = np.abs(t1_actual*(np.random.random(size=20)>(np.random.random()*.9+.05)).astype(int))\n",
    "print (\"actual   \", \", \".join([str(i) for i in t1_actual]))\n",
    "print (\"predicted\", \", \".join([str(i) for i in t1_predicted]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the value of your computation to the `accuracy` variable, **with three decimal places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = ...\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uncomment and execute the following lines if you want to see the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol = [97, 99, 99, 117, 114, 97, 99, 121, 32, 61, 32, 110, 112, 46, 109, 101, 97, 110, 40, 116, 49, 95, 97, 99, 116, 117, 97, 108, 61, 61, 116, 49, 95, 112, 114, 101, 100, 105, 99, 116, 101, 100, 41]\n",
    "# print (\"\".join([\"%c\"%i for i in sol]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Sensitivity\n",
    "\n",
    "Compute the sensitivity metric [aka the _True Positive Rate_ see [Sensitivity on Wikipedia](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)] for the following model output (`predicted`) and ground truth (`actual`)\n",
    "\n",
    "Execute the following cell to generate the data from which you must compute the metric. You may compute the metric implementing python code, or manually, or copy/pasting the actual and predicted data in Excel, etc.\n",
    "\n",
    "Observe that every time you execute the following cell, **a different set of values** is generated. You will have to compute the metric **for the values that you see**. If you run the cell again you will have to compute your metric value again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "t2_predicted = np.random.randint(2, size=20)\n",
    "t2_actual = np.random.randint(2, size=20)\n",
    "t2_predicted[np.argwhere(t2_actual==1)[0][0]]=0\n",
    "print (\"actual   \", \", \".join([str(i) for i in t2_actual]))\n",
    "print (\"predicted\", \", \".join([str(i) for i in t2_predicted]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the value of your computation to the `tpr` variable **with three decimal places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr =\n",
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uncomment and execute the following lines if you want to see the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol = [116, 112, 114, 32, 61, 32, 110, 112, 46, 109, 101, 97, 110, 40, 116, 50, 95, 97, 99, 116, 117, 97, 108, 91, 116, 50, 95, 112, 114, 101, 100, 105, 99, 116, 101, 100, 61, 61, 49, 93, 41]\n",
    "# print (\"\".join([\"%c\"%i for i in sol]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Evaluation in New York City Taxi Trip Duration Kaggle Competition\n",
    "\n",
    "Understand the data and the evaluation metric (**Root Mean Squared Logarithmic Error**, RMSLE) of the following Kaggle competition\n",
    "\n",
    "- https://www.kaggle.com/c/nyc-taxi-trip-duration/\n",
    "\n",
    "Observe that this competition is a **regression task** as we are measuring the difference in prediction with respect to the actual.\n",
    "\n",
    "For instance, the following model predictions and ground truth:\n",
    "\n",
    "    actual    [66 37 22]\n",
    "    predicted [79 51 67]\n",
    "    \n",
    "produce a **RMSLE** of 0.525 aprox.\n",
    "\n",
    "Execute the following cell to generate the data from which you must compute the metric. You may compute the metric implementing python code, or manually, or copy/pasting the actual and predicted data in Excel, etc.\n",
    "\n",
    "Observe that every time you execute the following cell, **a different set of values** is generated. You will have to compute the metric **for the values that you see**. If you run the cell again you will have to compute your metric value again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3_actual    = np.random.randint(80,size=15)+20\n",
    "t3_predicted = np.random.randint(80,size=15)+20\n",
    "print (\"actual   \", t3_actual)\n",
    "print (\"predicted\", t3_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the value of your computation to the `rmsle` variable **with three decimal places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsle = ...\n",
    "rmsle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uncomment and execute the following lines if you want to see the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol = [114, 109, 115, 108, 101, 32, 61, 32, 110, 112, 46, 115, 113, 114, 116, 40, 110, 112, 46, 109, 101, 97, 110, 40, 110, 112, 46, 108, 111, 103, 40, 116, 51, 95, 112, 114, 101, 100, 105, 99, 116, 101, 100, 43, 49, 41, 45, 110, 112, 46, 108, 111, 103, 40, 116, 51, 95, 97, 99, 116, 117, 97, 108, 43, 49, 41, 41, 42, 42, 50, 41]\n",
    "# print (\"\".join([\"%c\"%i for i in sol]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Evaluation in Shelter Animal Outcomes Kaggle Competition\n",
    "\n",
    "Understand the data and the evaluation metric (**Multiclass Logaritmic Loss**, _logloss_) of the following Kaggle competition\n",
    "\n",
    "- https://www.kaggle.com/c/shelter-animal-outcomes/\n",
    "\n",
    "Observe that this competition is a **classification task with 5 classes** and, for each item, the model produces a probability for each class. Classes are numbered from 0 to 4.\n",
    "\n",
    "For instance, the following represents the model output for **three items**\n",
    "\n",
    "    [[0.17 0.27 0.03 0.31 0.21]\n",
    "     [0.09 0.44 0.02 0.15 0.3 ]\n",
    "     [0.26 0.18 0.25 0.2  0.11]]\n",
    "     \n",
    "Where the classes with gretest probability assigned by the model are \n",
    "\n",
    "- class 3 for the first item (with 0.31 probability) \n",
    "- class 1 for the second item (with 0.44 probability)\n",
    "- class 0 for the third item (with 0.26 probability)\n",
    "\n",
    "The class labels are expressed as a similar matrix, but with 0/1\n",
    "For instance, the ground truth for the corresponding three items above, could be:\n",
    "\n",
    "    [[0 0 0 1 0]\n",
    "     [0 0 1 0 0]\n",
    "     [1 0 0 0 0]]\n",
    "\n",
    "and will produce a **logloss** of approx 2.14\n",
    "\n",
    "Execute the following cell to generate the data from which you must compute the metric. You may compute the metric implementing python code, or manually, or copy/pasting the actual and predicted data in Excel, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "t4_actual = np.random.random(size=(7,5)).T/4\n",
    "t4_actual = np.round((t4_actual/np.sum(t4_actual,axis=0)),2).T\n",
    "\n",
    "t4_predicted = np.eye(5)[np.random.randint(5,size=7)].astype(int)\n",
    "\n",
    "print (\"actual\")\n",
    "print (t4_actual)\n",
    "print (\"\\npredicted\")\n",
    "print (t4_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the value of your computation to the `logloss` variable **with three decimal places**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losloss = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uncomment and execute the following lines if you want to see the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol = [108, 111, 103, 108, 111, 115, 115, 32, 61, 32, 45, 110, 112, 46, 109, 101, 97, 110, 40, 110, 112, 46, 108, 111, 103, 40, 110, 112, 46, 115, 117, 109, 40, 116, 52, 95, 112, 114, 101, 100, 105, 99, 116, 101, 100, 42, 116, 52, 95, 97, 99, 116, 117, 97, 108, 44, 97, 120, 105, 115, 61, 49, 41, 41, 41]\n",
    "# print (\"\".join([\"%c\"%i for i in sol]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p37",
   "language": "python",
   "name": "p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
