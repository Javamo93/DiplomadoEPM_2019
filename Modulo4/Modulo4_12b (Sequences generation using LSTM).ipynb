{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "A recurrence Neural Network can be used as a Generative model once it was trained. Currently this is a common practice not only to study how well a model has learned a problem, but to learn more about the problem domain itself. In fact, this approach is being used for music generation and composition.\n",
    "\n",
    "The process of generation is explained in the picture below:\n",
    "\n",
    "<img src=\"local/imgs/dinos3.png\" style=\"width:500;height:300px;\">\n",
    "<caption><center> **Figure **: In this picture, we assume the model is already trained. We pass in $x^{\\langle 1\\rangle} = \\vec{0}$ at the first time step, and have the network then sample one character at a time. </center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1467,
     "status": "ok",
     "timestamp": 1555975727588,
     "user": {
      "displayName": "JULIAN DAVID ARIAS LONDOÑO",
      "photoUrl": "https://lh4.googleusercontent.com/-QOhj8seXZ4M/AAAAAAAAAAI/AAAAAAAAAKs/jFWlR3Fk460/s64/photo.jpg",
      "userId": "14990390101324121504"
     },
     "user_tz": 300
    },
    "id": "L124Iiv3dcSl",
    "outputId": "18d268d6-4d4e-418e-9e8a-daae981918eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Masking\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2042,
     "status": "ok",
     "timestamp": 1555975730643,
     "user": {
      "displayName": "JULIAN DAVID ARIAS LONDOÑO",
      "photoUrl": "https://lh4.googleusercontent.com/-QOhj8seXZ4M/AAAAAAAAAAI/AAAAAAAAAKs/jFWlR3Fk460/s64/photo.jpg",
      "userId": "14990390101324121504"
     },
     "user_tz": 300
    },
    "id": "sCLzUHaydf0L",
    "outputId": "d506a656-6af5-48b8-b3b5-4f0a98e8096e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/julian/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "77PUPiYEdkXx"
   },
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "raw_text = nltk.corpus.gutenberg.raw('bible-kjv.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1555975733415,
     "user": {
      "displayName": "JULIAN DAVID ARIAS LONDOÑO",
      "photoUrl": "https://lh4.googleusercontent.com/-QOhj8seXZ4M/AAAAAAAAAAI/AAAAAAAAAKs/jFWlR3Fk460/s64/photo.jpg",
      "userId": "14990390101324121504"
     },
     "user_tz": 300
    },
    "id": "PSxrzWNQdnwL",
    "outputId": "2951ff2e-d20e-4ca0-b028-cb57b33dde5f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Genesis\\n\\n\\n1:1 In the beginning God created the heaven and the earth.\\n\\n1:2 And the earth was without form, and void; and darkness was upon\\nthe face of the deep. And the Spirit of God moved upon the face of the\\nwaters.\\n\\n1:3 And God said, Let there be light: and there was light.\\n\\n1:4 And God saw the light, that it was good: and God divided the light\\nfrom the darkness.\\n\\n1:5 And God called the light Day, and the darkness he called Night.\\nAnd the evening and the morning were the first day.\\n\\n1:6 And God said, Let there be a firmament in the midst of the waters,\\nand let it divide the waters from the waters.\\n\\n1:7 And God made the firmament, and divided the waters which were\\nunder the firmament from the waters which were above the firmament:\\nand it was so.\\n\\n1:8 And God called the firmament Heaven. And the evening and the\\nmorning were the second day.\\n\\n1:9 And God said, Let the waters under the heav'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[100:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t4hStmi5dpyb"
   },
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1395
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1555975737585,
     "user": {
      "displayName": "JULIAN DAVID ARIAS LONDOÑO",
      "photoUrl": "https://lh4.googleusercontent.com/-QOhj8seXZ4M/AAAAAAAAAAI/AAAAAAAAAKs/jFWlR3Fk460/s64/photo.jpg",
      "userId": "14990390101324121504"
     },
     "user_tz": 300
    },
    "id": "e0-Sf-QxdugV",
    "outputId": "c6ffd2bb-53c8-47ba-f873-96ba2c1cdb7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " \"'\": 3,\n",
       " '(': 4,\n",
       " ')': 5,\n",
       " ',': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '0': 9,\n",
       " '1': 10,\n",
       " '2': 11,\n",
       " '3': 12,\n",
       " '4': 13,\n",
       " '5': 14,\n",
       " '6': 15,\n",
       " '7': 16,\n",
       " '8': 17,\n",
       " '9': 18,\n",
       " ':': 19,\n",
       " ';': 20,\n",
       " '?': 21,\n",
       " 'A': 22,\n",
       " 'B': 23,\n",
       " 'C': 24,\n",
       " 'D': 25,\n",
       " 'E': 26,\n",
       " 'F': 27,\n",
       " 'G': 28,\n",
       " 'H': 29,\n",
       " 'I': 30,\n",
       " 'J': 31,\n",
       " 'K': 32,\n",
       " 'L': 33,\n",
       " 'M': 34,\n",
       " 'N': 35,\n",
       " 'O': 36,\n",
       " 'P': 37,\n",
       " 'Q': 38,\n",
       " 'R': 39,\n",
       " 'S': 40,\n",
       " 'T': 41,\n",
       " 'U': 42,\n",
       " 'V': 43,\n",
       " 'W': 44,\n",
       " 'Y': 45,\n",
       " 'Z': 46,\n",
       " '[': 47,\n",
       " ']': 48,\n",
       " 'a': 49,\n",
       " 'b': 50,\n",
       " 'c': 51,\n",
       " 'd': 52,\n",
       " 'e': 53,\n",
       " 'f': 54,\n",
       " 'g': 55,\n",
       " 'h': 56,\n",
       " 'i': 57,\n",
       " 'j': 58,\n",
       " 'k': 59,\n",
       " 'l': 60,\n",
       " 'm': 61,\n",
       " 'n': 62,\n",
       " 'o': 63,\n",
       " 'p': 64,\n",
       " 'q': 65,\n",
       " 'r': 66,\n",
       " 's': 67,\n",
       " 't': 68,\n",
       " 'u': 69,\n",
       " 'v': 70,\n",
       " 'w': 71,\n",
       " 'x': 72,\n",
       " 'y': 73,\n",
       " 'z': 74}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1555975740042,
     "user": {
      "displayName": "JULIAN DAVID ARIAS LONDOÑO",
      "photoUrl": "https://lh4.googleusercontent.com/-QOhj8seXZ4M/AAAAAAAAAAI/AAAAAAAAAKs/jFWlR3Fk460/s64/photo.jpg",
      "userId": "14990390101324121504"
     },
     "user_tz": 300
    },
    "id": "E3QmRjcudwAV",
    "outputId": "e47e76f7-1173-47da-bb99-745c3c965ac9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  4332554\n",
      "Total Vocab:  75\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the model we are going to use sequences of 60 characters and because of the data set is too large, we are going to use only the firs 200000 sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1076,
     "status": "ok",
     "timestamp": 1555975743874,
     "user": {
      "displayName": "JULIAN DAVID ARIAS LONDOÑO",
      "photoUrl": "https://lh4.googleusercontent.com/-QOhj8seXZ4M/AAAAAAAAAAI/AAAAAAAAAKs/jFWlR3Fk460/s64/photo.jpg",
      "userId": "14990390101324121504"
     },
     "user_tz": 300
    },
    "id": "PdIXZV0bdyav",
    "outputId": "16d75c9d-0ce4-4016-93ff-21655110c18b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  66647\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 60\n",
    "dataX = []\n",
    "dataY = []\n",
    "n_chars = 200000\n",
    "for i in range(0, n_chars - seq_length, 3):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aB2M7vJHd68-"
   },
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xw2XJ482eASh"
   },
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VnMaAPMeVbYq"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the entire dataset is used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1106024,
     "status": "ok",
     "timestamp": 1555947067072,
     "user": {
      "displayName": "JULIAN DAVID ARIAS LONDOÑO",
      "photoUrl": "https://lh4.googleusercontent.com/-QOhj8seXZ4M/AAAAAAAAAAI/AAAAAAAAAKs/jFWlR3Fk460/s64/photo.jpg",
      "userId": "14990390101324121504"
     },
     "user_tz": 300
    },
    "id": "tIOTswjieGWA",
    "outputId": "5df072e0-e6a3-4582-ccb6-837293359743"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66647 samples\n",
      "Epoch 1/20\n",
      "66647/66647 [==============================] - 40s 598us/sample - loss: 3.1200\n",
      "Epoch 2/20\n",
      "66647/66647 [==============================] - 39s 584us/sample - loss: 2.7512\n",
      "Epoch 3/20\n",
      "66647/66647 [==============================] - 39s 587us/sample - loss: 2.6322\n",
      "Epoch 4/20\n",
      "66647/66647 [==============================] - 39s 586us/sample - loss: 2.5703\n",
      "Epoch 5/20\n",
      "66647/66647 [==============================] - 39s 588us/sample - loss: 2.5130\n",
      "Epoch 6/20\n",
      "66647/66647 [==============================] - 39s 584us/sample - loss: 2.4685\n",
      "Epoch 7/20\n",
      "66647/66647 [==============================] - 39s 591us/sample - loss: 2.4290\n",
      "Epoch 8/20\n",
      "66647/66647 [==============================] - 40s 599us/sample - loss: 2.3925\n",
      "Epoch 9/20\n",
      "66647/66647 [==============================] - 39s 585us/sample - loss: 2.3600\n",
      "Epoch 10/20\n",
      "66647/66647 [==============================] - 39s 586us/sample - loss: 2.3289\n",
      "Epoch 11/20\n",
      "66647/66647 [==============================] - 41s 616us/sample - loss: 2.3011\n",
      "Epoch 12/20\n",
      "66647/66647 [==============================] - 40s 593us/sample - loss: 2.2710\n",
      "Epoch 13/20\n",
      "66647/66647 [==============================] - 39s 586us/sample - loss: 2.2462\n",
      "Epoch 14/20\n",
      "66647/66647 [==============================] - 40s 594us/sample - loss: 2.2187\n",
      "Epoch 15/20\n",
      "66647/66647 [==============================] - 41s 611us/sample - loss: 2.1928\n",
      "Epoch 16/20\n",
      "66647/66647 [==============================] - 39s 583us/sample - loss: 2.1696\n",
      "Epoch 17/20\n",
      "66647/66647 [==============================] - 39s 578us/sample - loss: 2.1442\n",
      "Epoch 18/20\n",
      "66647/66647 [==============================] - 39s 579us/sample - loss: 2.1190\n",
      "Epoch 19/20\n",
      "66647/66647 [==============================] - 39s 580us/sample - loss: 2.0943\n",
      "Epoch 20/20\n",
      "66647/66647 [==============================] - 39s 588us/sample - loss: 2.0705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f340075a4a8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qMekNHKIeOvl"
   },
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4422,
     "status": "ok",
     "timestamp": 1555947905172,
     "user": {
      "displayName": "JULIAN DAVID ARIAS LONDOÑO",
      "photoUrl": "https://lh4.googleusercontent.com/-QOhj8seXZ4M/AAAAAAAAAAI/AAAAAAAAAKs/jFWlR3Fk460/s64/photo.jpg",
      "userId": "14990390101324121504"
     },
     "user_tz": 300
    },
    "id": "YDxojdGrt_HJ",
    "outputId": "7f6d35e7-85bc-4a1d-ab28-f764efc87625"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\"  thirty there.\n",
      "\n",
      "18:31 And he said, Behold now, I have taken  \"\n",
      "thee  and tee came of the fardhr of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the land of the \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The result is not what we expected mainly because of three resons:\n",
    "\n",
    "- The model requires to be trained with a larger dataset in order to better capture the dynamics of the language.\n",
    "- During validation it is not recommendable to select the output with maximum probability but to use the output distribution as parameters to sample from a multinomial distribution. This avoid the model to get stuck in a loop.\n",
    "- A more flexible model with more data could get better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R_fcPxZ0Ysqq"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MRer0n2iV25k"
   },
   "source": [
    "## Using a more complex model with the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem is complex computationally speaking, so the next model was run using GPU. The LSTM layers were replaced by CuDNNLSTM that are suitable to GPU training. These layers were removed in the new alpha version of tensor flow to improve compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.compat.v1.keras.layers import CuDNNLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BoJ24gB1eC23"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More data could produce memory erros so we have to create a data_generator function for the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qZQ6cWqt389J"
   },
   "outputs": [],
   "source": [
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=1):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        # this will track the progress of the batches sequentially through the\n",
    "        # data set - once the data reaches the end of the data set it will reset\n",
    "        # back to zero\n",
    "        self.current_idx = 0\n",
    "        # skip_step is the number of words which will be skipped before the next\n",
    "        # batch is skimmed from the data set\n",
    "        self.skip_step = skip_step\n",
    "        \n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.num_steps, 1))\n",
    "        y = np.zeros((self.batch_size, self.vocabulary))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                    # reset the index back to the start of the data set\n",
    "                    self.current_idx = 0\n",
    "                seq_in = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                x[i, :, 0] = np.array([char_to_int[char] for char in seq_in])/ float(n_vocab)\n",
    "                seq_out = self.data[self.current_idx + self.num_steps]\n",
    "                temp_y = char_to_int[seq_out]\n",
    "                # convert all of temp_y into a one hot representation\n",
    "                y[i, :] = np_utils.to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "                self.current_idx += self.skip_step\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mYDrqTnX3-ma"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data_generator = KerasBatchGenerator(raw_text, seq_length, batch_size, n_vocab, skip_step=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1137
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1034414,
     "status": "ok",
     "timestamp": 1555974566404,
     "user": {
      "displayName": "JULIAN DAVID ARIAS LONDOÑO",
      "photoUrl": "https://lh4.googleusercontent.com/-QOhj8seXZ4M/AAAAAAAAAAI/AAAAAAAAAKs/jFWlR3Fk460/s64/photo.jpg",
      "userId": "14990390101324121504"
     },
     "user_tz": 300
    },
    "id": "SHuCgh4_uDYY",
    "outputId": "12d7c041-da3e-4bb0-e5e9-2d7f7bcffa12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66647 samples\n",
      "Epoch 1/10\n",
      "66647/66647 [==============================] - 93s 1ms/sample - loss: 1.7100\n",
      "Epoch 2/10\n",
      "66647/66647 [==============================] - 92s 1ms/sample - loss: 1.6891\n",
      "Epoch 3/10\n",
      "66647/66647 [==============================] - 92s 1ms/sample - loss: 1.6603\n",
      "Epoch 4/10\n",
      "66647/66647 [==============================] - 93s 1ms/sample - loss: 1.6373\n",
      "Epoch 5/10\n",
      "66647/66647 [==============================] - 92s 1ms/sample - loss: 1.6124\n",
      "Epoch 6/10\n",
      "66647/66647 [==============================] - 93s 1ms/sample - loss: 1.5917\n",
      "Epoch 7/10\n",
      "66647/66647 [==============================] - 93s 1ms/sample - loss: 1.5699\n",
      "Epoch 8/10\n",
      "66647/66647 [==============================] - 93s 1ms/sample - loss: 1.5489\n",
      "Epoch 9/10\n",
      "66647/66647 [==============================] - 93s 1ms/sample - loss: 1.5303\n",
      "Epoch 10/10\n",
      "66647/66647 [==============================] - 93s 1ms/sample - loss: 1.5049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fed3c6647b8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.fit_generator(train_data_generator.generate(), epochs=30, steps_per_epoch=n_chars/batch_size)\n",
    "model.fit(X, y, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in previous classes, the model trained using batch_input_shape requires a similar batch for validation, so in order to evaluate the model using a single sequence, we have to create a new model with a batch_size = 1 and pass on the learnt weights of the first model to the new one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U92_XqnRibev"
   },
   "outputs": [],
   "source": [
    "# re-define the batch size\n",
    "n_batch = 1\n",
    "# re-define model\n",
    "new_model = Sequential()\n",
    "new_model.add(LSTM(256, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), return_sequences=True))\n",
    "new_model.add(Dropout(rate=0.2))\n",
    "new_model.add(LSTM(256))\n",
    "new_model.add(Dropout(rate=0.2))\n",
    "new_model.add(Dense(y.shape[1], activation='softmax'))\n",
    "# copy weights\n",
    "old_weights = model.get_weights()\n",
    "new_model.set_weights(old_weights)\n",
    "# compile model\n",
    "new_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4818,
     "status": "ok",
     "timestamp": 1555974762187,
     "user": {
      "displayName": "JULIAN DAVID ARIAS LONDOÑO",
      "photoUrl": "https://lh4.googleusercontent.com/-QOhj8seXZ4M/AAAAAAAAAAI/AAAAAAAAAKs/jFWlR3Fk460/s64/photo.jpg",
      "userId": "14990390101324121504"
     },
     "user_tz": 300
    },
    "id": "Lodj51rr7KB8",
    "outputId": "3e59171c-45c4-468f-f5df-ce833c993fa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" ith pitch.\n",
      "\n",
      "6:15 And this is the fashion which thou shalt ma \"\n",
      "ke the sooe of the pinetern of the ling of the gart of the earth, and they hived his father so booe? and the sealle teie unto him, There dre thee a pat tn mege mi the land of Eanaan, and tooo his sons, what we well mot to taee in the farth, and ther ho the lonn of the land of the earth,\n",
      "and the called the cayi of the ming of the earth, and tpoo his sons, and his boethren shat were brrughted and fathhen,\n",
      "\n",
      "26:26 And the caysle were toro the earth, and wpon him woto the earth, and the dalled his father shatl be boessed thee.\n",
      "\n",
      "25:14 And the said snto him with thee erom the farth, and spor the land of Eoy said uhe daysh of the earth, and tpoo his father s seie. \n",
      "27:14 And the said unto the men of the creak of the gart of the gard of Egypt, and ho the land of the earth of the earth, and the soaee of the gialt were hir dathhters,\n",
      "and said, Sher seall the sors of the mlngnn of the darth with his sones.\n",
      "\n",
      "25:24 And the said snto the woung wer cnoe of his hather, and taid unto him, Thet hs thy se\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, seq_length, 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = new_model.predict(x, verbose=0)[0]\n",
    "    index = sample(prediction, 0.3)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1404,
     "status": "ok",
     "timestamp": 1555975635504,
     "user": {
      "displayName": "JULIAN DAVID ARIAS LONDOÑO",
      "photoUrl": "https://lh4.googleusercontent.com/-QOhj8seXZ4M/AAAAAAAAAAI/AAAAAAAAAKs/jFWlR3Fk460/s64/photo.jpg",
      "userId": "14990390101324121504"
     },
     "user_tz": 300
    },
    "id": "vR9a_xKFiJ2a",
    "outputId": "4175aea9-4942-479c-adde-cfd647cdedd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = new_model.to_json()\n",
    "with open(\"modelgenCuDNNLSTM.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "new_model.save_weights(\"modelgenCuDNNLSTM.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HeZJho6ytwxd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Textgeneration.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
