{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-cache -O init.py -q https://raw.githubusercontent.com/jdariasl/DiplomadoEPM_2019/master/Modulo3/init.py\n",
    "from init import init; init(force_download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    print (\"setting tensorflow version in colab\")\n",
    "    %tensorflow_version 2.x\n",
    "    %load_ext tensorboard\n",
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(1000, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[ 0.04910969, -0.04948508,  0.01833928, -0.00107753,  0.00912641],\n",
       "       [-0.03303789, -0.00559405, -0.00318743,  0.01124633, -0.02370098],\n",
       "       [-0.04715276,  0.00160822,  0.02365217, -0.02993683, -0.0436941 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(tf.constant([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 5), dtype=float32, numpy=\n",
       "array([[[ 0.0258058 , -0.04209884, -0.03447083,  0.02910941,\n",
       "          0.0227241 ],\n",
       "        [ 0.04910969, -0.04948508,  0.01833928, -0.00107753,\n",
       "          0.00912641],\n",
       "        [-0.03303789, -0.00559405, -0.00318743,  0.01124633,\n",
       "         -0.02370098]],\n",
       "\n",
       "       [[-0.04715276,  0.00160822,  0.02365217, -0.02993683,\n",
       "         -0.0436941 ],\n",
       "        [-0.01106826,  0.04215386, -0.01418646,  0.02713865,\n",
       "         -0.04123275],\n",
       "        [-0.01895002, -0.0272177 ,  0.02751021,  0.01902193,\n",
       "          0.00526396]]], dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(tf.constant([[0,1,2],[3,4,5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras import preprocessing\n",
    "max_features = 100000\n",
    "maxlen = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi = imdb.get_word_index()\n",
    "iw = {v:k for k,v in wi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 218 1 [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25]\n",
      "1 189 0 [1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14]\n",
      "2 141 0 [1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14]\n",
      "3 550 1 [1, 4, 18609, 16085, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61]\n",
      "4 147 0 [1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626]\n",
      "5 43 0 [1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, 43222, 32, 85, 156, 45, 40, 148, 139]\n",
      "6 123 1 [1, 6740, 365, 1234, 5, 1156, 354, 11, 14, 5327, 6638, 7, 1016, 10626, 5940, 356, 44, 4, 1349, 500]\n",
      "7 562 0 [1, 4, 14906, 716, 4, 65, 7, 4, 689, 4367, 6308, 2343, 4804, 28674, 84206, 5270, 32099, 2315, 71688, 12572]\n",
      "8 233 1 [1, 43, 188, 46, 5, 566, 264, 51, 6, 530, 664, 14, 9, 1713, 81, 25, 1135, 46, 7, 6]\n",
      "9 130 0 [1, 14, 20, 47, 111, 439, 3445, 19, 12, 15, 166, 12, 216, 125, 40, 6, 364, 352, 707, 1187]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print (i, len(x_train[i]), y_train[i], x_train[i][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'lets', 'loves', 'their', 'becomes', 'reaching', 'had', 'journalist', 'of', 'lot', 'from', 'anyone', 'to', 'have']\n",
      "1 ['the', 'thought', 'solid', 'thought', 'senator', 'do', 'making', 'to', 'is', 'spot', 'nomination', 'assumed', 'while', 'he', 'of', 'jack', 'in', 'where', 'picked', 'as']\n",
      "2 ['the', 'as', 'there', 'in', 'at', 'by', 'br', 'of', 'sure', 'many', 'br', 'of', 'proving', 'no', 'only', 'women', 'was', 'than', \"doesn't\", 'as']\n",
      "3 ['the', 'of', 'bernadette', 'mon', 'they', 'halfway', 'of', 'identity', 'went', 'plot', 'actors', 'watch', 'of', 'share', 'was', 'well', 'these', 'can', 'this', 'only']\n",
      "4 ['the', 'sure', 'themes', 'br', 'only', 'acting', 'i', 'i', 'was', 'favourite', 'as', 'on', 'she', 'they', 'hat', 'but', 'already', 'most', 'was', 'scares']\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print (i, [iw[j] for j in x_train[i][:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "xx_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 20)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print (x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  65,   16,   38, 1334,   88,   12,   16,  283,    5,   16, 4472,\n",
       "        113,  103,   32,   15,   16, 5345,   19,  178,   32], dtype=int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 20, 32)            3200000   \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 250)               160250    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 3,360,501\n",
      "Trainable params: 3,360,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Flatten, Dense, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(max_features, 32, input_length=maxlen)\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 12s 600us/sample - loss: 0.5573 - acc: 0.6945 - val_loss: 0.4978 - val_acc: 0.7518\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 14s 679us/sample - loss: 0.3776 - acc: 0.8333 - val_loss: 0.5218 - val_acc: 0.7418\n",
      "Epoch 3/10\n",
      "10400/20000 [==============>...............] - ETA: 6s - loss: 0.2276 - acc: 0.9098"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(xx_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       "array([-0.14986037, -0.16729084, -0.01940007, -0.01645998,  0.00436411,\n",
       "        0.03632338,  0.02847637, -0.04293869], dtype=float32)>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecar = embedding_layer(wi[\"car\"]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "etruck = embedding_layer(wi[\"truck\"]).numpy()\n",
    "ecat = embedding_layer(wi[\"cat\"]).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36207294"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(ecar-etruck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6426238"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(ecar-ecat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34261662"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(etruck-ecat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 20, 8)             800000    \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 20, 32)            800       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 10, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               80250     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 881,301\n",
      "Trainable params: 881,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(max_features, 8, input_length=maxlen)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 4s 177us/sample - loss: 0.5690 - acc: 0.6852 - val_loss: 0.5033 - val_acc: 0.7374\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 3s 154us/sample - loss: 0.4342 - acc: 0.7957 - val_loss: 0.4878 - val_acc: 0.7576\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 3s 159us/sample - loss: 0.3847 - acc: 0.8274 - val_loss: 0.5038 - val_acc: 0.7564\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 3s 150us/sample - loss: 0.3501 - acc: 0.8466 - val_loss: 0.4966 - val_acc: 0.7514\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 3s 166us/sample - loss: 0.3159 - acc: 0.8645 - val_loss: 0.5417 - val_acc: 0.7496\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 4s 194us/sample - loss: 0.2794 - acc: 0.8836 - val_loss: 0.5401 - val_acc: 0.7510\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 5s 251us/sample - loss: 0.2387 - acc: 0.9018 - val_loss: 0.5892 - val_acc: 0.7522\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 5s 241us/sample - loss: 0.1980 - acc: 0.9220 - val_loss: 0.6789 - val_acc: 0.7292\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 5s 248us/sample - loss: 0.1631 - acc: 0.9367 - val_loss: 0.6972 - val_acc: 0.7402\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 4s 220us/sample - loss: 0.1330 - acc: 0.9507 - val_loss: 0.7826 - val_acc: 0.7408\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(xx_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p37",
   "language": "python",
   "name": "p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
